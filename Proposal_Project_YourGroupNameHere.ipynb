{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 - Project Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "You have the choice of doing either (1) an AI solve a problem style project or (2) run a Special Topics class on a topic of your choice.  If you want to do (2) you should fill out the _other_ proposal for that. This is the proposal description for (1).\n",
    "\n",
    "You will design and execute a machine learning project. There are a few constraints on the nature of the allowed project. \n",
    "- The problem addressed will not be a \"toy problem\" or \"common training students problem\" like 8-Queens or a small Traveling Salesman Problem or similar\n",
    "- If its the kind of problem (e.g., RL) that interacts with a simulator or live task, then the problem will have a reasonably complex action space. For instance, a wupus world kind of thing with a 9x9 grid is definitely too small.  A simulated mountain car with a less complex 2-d road and simplified dynamics seems like a fairly low achievement level.  A more complex 3-d mountain car simulation with large extent and realistic dynamics, sure sounds great!\n",
    "- If its the kind of problem that uses a dataset, then the dataset will have >1k observations and >5 variables. I'd prefer more like >10k observations and >10 variables. A general rule is that if you have >100x more observations than variables, your solution will likely generalize a lot better. The goal of training an unsupervised machine learning model is to learn the underlying pattern in a dataset in order to generalize well to unseen data, so choosing a large dataset is very important.\n",
    "- The project must include some elements we talked about in the course\n",
    "- The project will include a model selection and/or feature selection component where you will be looking for the best setup to maximize the performance of your AI system. Generally RL tasks may require a huge amount of training, so extensive grid search is unlikely to be possible. However expoloring a few reasonable hyper-parameters may still be possible. \n",
    "- You will evaluate the performance of your AI system using more than one appropriate metric\n",
    "- You will be writing a report describing and discussing these accomplishments\n",
    "\n",
    "\n",
    "Feel free to delete this description section when you hand in your proposal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "Hopefully your team is at least this good. Obviously you should replace these with your names.\n",
    "\n",
    "- Hyeonseo (Heather) Jang\n",
    "- Bonan (Jack) Jia\n",
    "- Esha Kakarla\n",
    "- Kevin Tran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "The goal of this project is to train a model to optimize Battleship play by developing a strategy that maximizes the efficiency of targeting and sinking enemy ships. Collection of data will be conducted through randomizing the spawns of each ship and have the model learn after a certain rounds of games. With the data, we will train a machine learning model to predict optimal moves and evaluate different strategies. The model performance will be assessed through measuring the number of moves it took to win and overall win rate. Furthermore, we will explore how different rewards can affect the performance.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Battleship is a popular naval strategy game that is somewhat simple but has served as a testing ground for algorithms involving AI development and Reinforcement Learning. Battleship is a great well established environment for reinforcement learning because it encompasses a mix of strategic decisions, sequential actions, and hidden information between players <sup><a href=\"#footnote1\">[1]</a></sup>.\n",
    "\n",
    "The paper Reinforcement Learning for the Game of Battleship explores reinforcement learning algorithms in battleship. This paper analyzes certain challenges with the game's partially observable environment where the model has to to deduct the locations of the \n",
    "opponent's ships through several test-runs. This paper also explores heuristic search algorithms and analyzes the performances between the decision making processes <sup><a href=\"#footnote1\">[2]</a></sup>.\n",
    "\n",
    "Another article titled, An Artificial Intelligence Learns to Play Battleship on Medium, discusses the method of reinforcement learning for a game of battleship. This project has two main approaches, code from scratch using a linear model and using openAi gym library with neural networks. This author implemented q-learning to improve the agent's decision making over 100,000 training episodes<sup><a href=\"#footnote1\">[3]</a></sup>.\n",
    "\n",
    "Developing an AI approach to play battleship can involve several different types of algorithms to locate ships and stragetize. Common approaches are Monte Carlo Tree Search, Reinforcement Learning, and probability density function. By leveraging these algorithms, the search method can be refined over time and improve decision making <sup><a href=\"#footnote1\">[4]</a></sup>.\n",
    "\n",
    "The article, Coding an Intelligent Battleship Agent, discusses creating an agent to solve the Battleship game, from simple random guessing to advanced approaches such as a Hunt/Target strategy and probabilistic strategies. The agent has learned to compile shots based on probabilities improving overall efficiency <sup><a href=\"#footnote1\">[5]</a></sup>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The problem we are trying to solve is: “How can we develop an optimal strategy for winning the Battleship game using machine learning?” The winner of Battleship is the first person who sinks all of the enemy ships with the least number of actions taken. As such, we are exploring how to optimize the number of actions taken to win the game. The agent’s action space includes only attacking specific grid cells on the board. A reward function will assign positive rewards for successfully hitting and sinking enemy ships, and negative rewards for misses. The probability of hit can be estimated using various machine learning methods such as Monte Carlo and reinforcement learning. The performance can be measured by win and lose rates, as well as the total number of moves the model made. We will repeat simulating the game multiple times to create dataset, making this problem replicable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We will create our own dataset by simulating Battleship game with variables such as 'models used', 'episodes', 'cost of action', 'cost of misses', and 'reward for hitting'. The critical variables include 'reward received' and 'move selection'. 'Reward received' is going to be represented with the numerical values calculated with reward equation. 'Move selections' are going to be represented with x and y coordinates. The total number of observations is going to be around 3000. The hits for each episode and graphs will be used to visualize the improvement of the model.\n",
    "\n",
    "We are unsure if we will need special handling, transformations, and handling, as we have not produced our data yet, but it will become noticeable whether they are necessary or not as we are planning to create tables and graphs to record and visualize our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "Our proposed solution is to create an algorithm that best can perform best when playing \"Battleship\". We will test other models to test their performance on a game that highlights heuristics. Alternative machine learning algorithms may perform [state] on the board game \"Battleships\". We aim to explore different reward methods, heuristic methods, and parameters that can affect the actions taken by the model compared against alternative models. This exploration dives into Monte Carlo and policy evaluation approach to improve and calcuate the best moves given the current state in Battleship. We aim to also run multiple iterations to view how policy and reward functions change over time.\n",
    "\n",
    "We want to explore reinforcement learning through Battleship since each ship can be placed any grid in the given boundary and in any orientation, it is up to the player to determine where best to place them such that the opponent will take longer (eventually finding the location of the ships) steps than the player. Given a player verus player scenario, the actions take is up to the player's understanding and heuristic of to win the game (where the player sinks all enemy ships). We also hope to explore the patterns that may arise in human performance when placing down ships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "### Cumulative Reward\n",
    "- For each episode, calculate the total reward accumulated by the agent from start to finish.\n",
    "- Plot the total rewards over episodes to evaluate whether the agent is improving its performance over time.\n",
    "\n",
    "### Average Run Reward\n",
    "- Track the total reward for each episode.\n",
    "- Compute the average of rewards over a fixed number of episodes.\n",
    "\n",
    "### Win Rate or Success Rate (If limiting number of episodes)\n",
    "- Track the number of games won by the agent.\n",
    "- Divide by the total number of games played to get the win rate.\n",
    "\n",
    "### Maximum Reward\n",
    "- The highest possible reward achievable in an episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our potential concerns root from data security, honest representation, miss application, and metric selection. We will keep our data secure by encrypting stored game data and AI models to prevent unauthorized access. Furthermore, we will honesty represent our data by checking for unrealistic ship placement. To avoid miss application, where the model trained on Battleship has a possibility of adapting for other purposes, we will clearly document that the model is intended only for Battleship strategy optimization. Lastly, to select the most relevant metrics for our problem, we will carefully design our reward function, making sure the model is prioritizing actions that lead to a win as well as making efficient moves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put things here that cement how you will interact/communicate as a team, how you will handle conflict and difficulty, how you will handle making decisions and setting goals/schedule, how much work you expect from each other, how you will handle deadlines, etc...\n",
    "* Attend weekly meetings\n",
    "* Be respectful to each other\n",
    "* Actively communicate on Discord\n",
    "* Complete given tasks before deadline\n",
    "* Open communication in general and especially with blockers\n",
    "* Provide open feedback to group members' work\n",
    "* Be accountable of your portion of work\n",
    "* Ask for help if encountering issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 2/9  |  2 PM |  Introductions, Communications  | Decide which form of project, Project Timeline, Brainstorming topics, Decide topic, Allocate work | \n",
    "| 2/12  |  6 PM |  Allocate work  | Update Data, Ethics & Privacy sections, work on project proposal notebook | \n",
    "| 2/16  |  2 PM |  work on project proposal notebook | Update Data, Ethics & Privacy sections, work on project proposal notebook  | \n",
    "| 2/23  | 2 PM  | Edit, finalize, and submit proposal | Discuss Wrangling and possible analytical approaches; Assign group members to lead each specific part   |\n",
    "| 3/2  | 2 PM  | Import & Wrangle Data ,do some EDA (Maradonna) | Review/Edit wrangling/EDA; Discuss Analysis Plan   |\n",
    "| 3/9  | 2 PM  | Finalize wrangling/EDA; Begin programming for project (Cruyff) | Discuss/edit project code; Complete project |\n",
    "| 3/16  | 2 PM  | Complete analysis; Draft results/conclusion/discussion (Carlos)| Discuss/edit full project |\n",
    "| 3/19  | Before 11:59 PM  | NA | Turn in Final Project  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<p id=\"footnote1\">[1] <a href=\"https://www.ga-ccri.com/deep-reinforcement-learning-win-battleship\" target=\"_blank\">He, S. (2017). *Deep Reinforcement Learning–of how to win at Battleship*. GA-CCRi</a>.</p>\n",
    "\n",
    "<p id=\"footnote1\">[2] <a href=\"https://is.muni.cz/th/oupp1/Reinforcement_Learning_for_the_Game_of_Battleship.pdf\" target=\"_blank\">Šebek, P. (2020). *Reinforcement Learning for the Game of Battleship*. Masaryk University</a>.</p>\n",
    "\n",
    "<p id=\"footnote1\">[3] <a href=\"https://medium.com/towards-data-science/an-artificial-intelligence-learns-to-play-battleship-ebd2cf9adb01\" target=\"_blank\">An Artificial Intelligence Learns to Play Battleship</a>.</p>\n",
    "\n",
    "<p id=\"footnote1\">[4] <a href=https://www.geeksforgeeks.org/play-battleships-game-with-ai/#1-probability-density-function target=\"_blank\">Play Battleships Game with AI: Insights into Algorithmic Strategies and Game Mastery</a>.</p>\n",
    "\n",
    "<p id=\"footnote1\">[5] <a href=https://towardsdatascience.com/coding-an-intelligent-battleship-agent-bf0064a4b319/ target=\"_blank\">Coding an Intelligent Battleship Agent</a>.</p> \n",
    "\n",
    "<a name=\"admonishnote\"></a>2.[^](#admonish): Also refs should be important to the background, not some randomly chosen vaguely related stuff. Include a web link if possible in refs as above.<br>\n",
    "<a name=\"sotanote\"></a>3.[^](#sota): Perhaps the current state of the art solution such as you see on [Papers with code](https://paperswithcode.com/sota). Or maybe not SOTA, but rather a standard textbook/Kaggle solution to this kind of problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11 (default, Jul 27 2021, 07:03:16) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
