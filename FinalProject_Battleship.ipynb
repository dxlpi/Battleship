{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118B - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Battleship\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Hyeonseo (Heather) Jang\n",
    "- Bonan (Jack) Jia\n",
    "- Esha Kakarla\n",
    "- Kevin Tran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "This section should be short and clearly stated. It should be a single paragraph <200 words.  It should summarize: \n",
    "- what your goal/problem is\n",
    "- what the data used represents \n",
    "- the solution/what you did\n",
    "- major results you came up with (mention how results are measured) \n",
    "\n",
    "__NB:__ this final project form is much more report-like than the proposal and the checkpoint. Think in terms of writing a paper with bits of code in the middle to make the plots/tables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Battleship is a popular naval strategy game that is somewhat simple but has served as a testing ground for algorithms involving AI development and Reinforcement Learning. Battleship is a great well established environment for reinforcement learning because it encompasses a mix of strategic decisions, sequential actions, and hidden information between players <sup><a href=\"#footnote1\">[1]</a></sup>.\n",
    "\n",
    "The paper Reinforcement Learning for the Game of Battleship explores reinforcement learning algorithms in battleship. This paper analyzes certain challenges with the game's partially observable environment where the model has to to deduct the locations of the \n",
    "opponent's ships through several test-runs. This paper also explores heuristic search algorithms and analyzes the performances between the decision making processes <sup><a href=\"#footnote1\">[2]</a></sup>.\n",
    "\n",
    "Another article titled, An Artificial Intelligence Learns to Play Battleship on Medium, discusses the method of reinforcement learning for a game of battleship. This project has two main approaches, code from scratch using a linear model and using openAi gym library with neural networks. This author implemented q-learning to improve the agent's decision making over 100,000 training episodes<sup><a href=\"#footnote1\">[3]</a></sup>.\n",
    "\n",
    "Developing an AI approach to play battleship can involve several different types of algorithms to locate ships and stragetize. Common approaches are Monte Carlo Tree Search, Reinforcement Learning, and probability density function. By leveraging these algorithms, the search method can be refined over time and improve decision making <sup><a href=\"#footnote1\">[4]</a></sup>.\n",
    "\n",
    "The article, Coding an Intelligent Battleship Agent, discusses creating an agent to solve the Battleship game, from simple random guessing to advanced approaches such as a Hunt/Target strategy and probabilistic strategies. The agent has learned to compile shots based on probabilities improving overall efficiency <sup><a href=\"#footnote1\">[5]</a></sup>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The problem we are trying to solve is: “How can we develop an optimal strategy for winning the Battleship game using machine learning?” The winner of Battleship is the first person who sinks all of the enemy ships with the least number of actions taken. As such, we are exploring how to optimize the number of actions taken to win the game. The agent’s action space includes only attacking specific grid cells on the board. A reward function will assign positive rewards for successfully hitting and sinking enemy ships, and negative rewards for misses. The probability of hit can be estimated using various machine learning methods such as Monte Carlo and reinforcement learning. The performance can be measured by win and lose rates, as well as the total number of moves the model made. We will repeat simulating the game multiple times to create dataset, making this problem replicable.\n",
    "\n",
    "**comments**: Make it more mathematical and precise (ie. describe the reward function mathematically using an equation, describe the agent's stae and action spce completely in mathematical terms.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We will create our own dataset by simulating Battleship game with variables such as 'models used', 'episodes', 'cost of action', 'cost of misses', and 'reward for hitting'. The critical variables include 'reward received' and 'move selection'. 'Reward received' is going to be represented with the numerical values calculated with reward equation. 'Move selections' are going to be represented with x and y coordinates. The total number of observations is going to be around 3000. The hits for each episode and graphs will be used to visualize the improvement of the model.\n",
    "\n",
    "We are unsure if we will need special handling, transformations, and handling, as we have not produced our data yet, but it will become noticeable whether they are necessary or not as we are planning to create tables and graphs to record and visualize our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "Our proposed solution is to create an algorithm that can perform best when playing the board game, Battleship. We aim to test different reward, action, and state parameters to explore the best or optimal path to win a game of Battleship. This exploration dives into Monte Carlo and policy evaluation approach to improve and calcuate the best moves given the current state in Battleship. Each action take results in a change in the reward function which in turn will determine the next action taken by the model. Over multilate iterations of the model playing Battleships, we aim to view its performance--whether is has won or has found the optimal actions to win a game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "### Cumulative Reward\n",
    "- For each episode, calculate the total reward accumulated by the agent from start to finish.\n",
    "- Plot the total rewards over episodes to evaluate whether the agent is improving its performance over time.\n",
    "\n",
    "### Average Run Reward\n",
    "- Track the total reward for each episode.\n",
    "- Compute the average of rewards over a fixed number of episodes.\n",
    "\n",
    "### Win Rate or Success Rate (If limiting number of episodes)\n",
    "- Track the number of games won by the agent.\n",
    "- Divide by the total number of games played to get the win rate.\n",
    "\n",
    "### Maximum Reward\n",
    "- The highest possible reward achievable in an episode."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "You may have done tons of work on this. Not all of it belongs here. \n",
    "\n",
    "Reports should have a __narrative__. Once you've looked through all your results over the quarter, decide on one main point and 2-4 secondary points you want us to understand. Include the detailed code and analysis results of those points only; you should spend more time/code/plots on your main point than the others.\n",
    "\n",
    "If you went down any blind alleys that you later decided to not pursue, please don't abuse the TAs time by throwing in 81 lines of code and 4 plots related to something you actually abandoned.  Consider deleting things that are not important to your narrative.  If its slightly relevant to the narrative or you just want us to know you tried something, you could keep it in by summarizing the result in this report in a sentence or two, moving the actual analysis to another file in your repo, and providing us a link to that file.\n",
    "\n",
    "### Subsection 1\n",
    "\n",
    "You will likely have different subsections as you go through your report. For instance you might start with an analysis of the dataset/problem and from there you might be able to draw out the kinds of algorithms that are / aren't appropriate to tackle the solution.  Or something else completely if this isn't the way your project works.\n",
    "\n",
    "### Subsection 2\n",
    "\n",
    "Another likely section is if you are doing any feature selection through cross-validation or hand-design/validation of features/transformations of the data\n",
    "\n",
    "### Monte Carlo\n",
    "\n",
    "WIP\n",
    "\n",
    "### Monte Carlo With Different Hyperparameters\n",
    "\n",
    "By decreasing epsilion over time from ε = 0.1, increasing gamma (0.95-0.99), and increasing number of episodes (500-1000), a slight improvement of rewards has been shown. \n",
    "\n",
    "### Subsection 5 \n",
    "\n",
    "Maybe you do model selection again, but using a different kind of metric than before?  Or you compare a completely different approach/alogirhtm to the problem? Whatever, this stuff is just serving suggestions.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Are there any problems with the work?  For instance would more data change the nature of the problem? Would it be good to explore more hyperparams than you had time for?   \n",
    "\n",
    "\n",
    "### Future work\n",
    "Looking at the limitations and/or the toughest parts of the problem and/or the situations where the algorithm(s) did the worst... is there something you'd like to try to make these better.\n",
    "\n",
    "### Ethics & Privacy\n",
    "Our potential concerns root from data security, honest representation, miss application, and metric selection. We will keep our data secure by encrypting stored game data and AI models to prevent unauthorized access. Furthermore, we will honesty represent our data by checking for unrealistic ship placement. To avoid miss application, where the model trained on Battleship has a possibility of adapting for other purposes, we will clearly document that the model is intended only for Battleship strategy optimization. Lastly, to select the most relevant metrics for our problem, we will carefully design our reward function, making sure the model is prioritizing actions that lead to a win as well as making efficient moves. \n",
    "\n",
    "**comments**: If extend to more complex strategic games, could similar AI be used for automated cheating in multiplayer games? While Battleship is relatively simple, AI-assisted cheating is an theical concern in competitive gaming.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<p id=\"footnote1\">[1] <a href=\"https://www.ga-ccri.com/deep-reinforcement-learning-win-battleship\" target=\"_blank\">He, S. (2017). *Deep Reinforcement Learning–of how to win at Battleship*. GA-CCRi</a>.</p>\n",
    "\n",
    "<p id=\"footnote1\">[2] <a href=\"https://is.muni.cz/th/oupp1/Reinforcement_Learning_for_the_Game_of_Battleship.pdf\" target=\"_blank\">Šebek, P. (2020). *Reinforcement Learning for the Game of Battleship*. Masaryk University</a>.</p>\n",
    "\n",
    "<p id=\"footnote1\">[3] <a href=\"https://medium.com/towards-data-science/an-artificial-intelligence-learns-to-play-battleship-ebd2cf9adb01\" target=\"_blank\">An Artificial Intelligence Learns to Play Battleship</a>.</p>\n",
    "\n",
    "<p id=\"footnote1\">[4] <a href=https://www.geeksforgeeks.org/play-battleships-game-with-ai/#1-probability-density-function target=\"_blank\">Play Battleships Game with AI: Insights into Algorithmic Strategies and Game Mastery</a>.</p>\n",
    "\n",
    "<p id=\"footnote1\">[5] <a href=https://towardsdatascience.com/coding-an-intelligent-battleship-agent-bf0064a4b319/ target=\"_blank\">Coding an Intelligent Battleship Agent</a>.</p> \n",
    "\n",
    "<a name=\"admonishnote\"></a>2.[^](#admonish): Also refs should be important to the background, not some randomly chosen vaguely related stuff. Include a web link if possible in refs as above.<br>\n",
    "<a name=\"sotanote\"></a>3.[^](#sota): Perhaps the current state of the art solution such as you see on [Papers with code](https://paperswithcode.com/sota). Or maybe not SOTA, but rather a standard textbook/Kaggle solution to this kind of problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11 (default, Jul 27 2021, 07:03:16) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
