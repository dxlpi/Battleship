{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Battleship\n",
    "[Group \"Battleship\" Repo](https://github.com/dxlpi/Battleship)\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Hyeonseo (Heather) Jang\n",
    "- Bonan (Jack) Jia\n",
    "- Esha Kakarla\n",
    "- Kevin Tran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "The goal of this project is to train a model to optimize Battleship play by developing a strategy that maximizes the efficiency of targeting and sinking enemy ships. Collection of data will be conducted through randomizing the spawns of each ship and have the model learn after a certain rounds of games. With the data, we will train a machine learning model to predict optimal moves and evaluate different strategies. The model performance will be assessed through measuring the number of moves it took to win and overall win rate. Furthermore, we will explore how different rewards can affect the performance.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Battleship is a popular naval strategy game that is somewhat simple but has served as a testing ground for algorithms involving AI development and Reinforcement Learning. Battleship is a great well established environment for reinforcement learning because it encompasses a mix of strategic decisions, sequential actions, and hidden information between players <sup><a href=\"#footnote1\">[1]</a></sup>.\n",
    "\n",
    "The paper Reinforcement Learning for the Game of Battleship explores reinforcement learning algorithms in battleship. This paper analyzes certain challenges with the game's partially observable environment where the model has to to deduct the locations of the \n",
    "opponent's ships through several test-runs. This paper also explores heuristic search algorithms and analyzes the performances between the decision making processes <sup><a href=\"#footnote1\">[2]</a></sup>.\n",
    "\n",
    "Another article titled, An Artificial Intelligence Learns to Play Battleship on Medium, discusses the method of reinforcement learning for a game of battleship. This project has two main approaches, code from scratch using a linear model and using openAi gym library with neural networks. This author implemented q-learning to improve the agent's decision making over 100,000 training episodes<sup><a href=\"#footnote1\">[3]</a></sup>.\n",
    "\n",
    "Developing an AI approach to play battleship can involve several different types of algorithms to locate ships and stragetize. Common approaches are Monte Carlo Tree Search, Reinforcement Learning, and probability density function. By leveraging these algorithms, the search method can be refined over time and improve decision making <sup><a href=\"#footnote1\">[4]</a></sup>.\n",
    "\n",
    "The article, Coding an Intelligent Battleship Agent, discusses creating an agent to solve the Battleship game, from simple random guessing to advanced approaches such as a Hunt/Target strategy and probabilistic strategies. The agent has learned to compile shots based on probabilities improving overall efficiency <sup><a href=\"#footnote1\">[5]</a></sup>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The problem we are trying to solve is: “How can we develop an optimal strategy for winning the Battleship game using machine learning?” The winner of Battleship is the first person who sinks all of the enemy ships with the least number of actions taken. As such, we are exploring how to optimize the number of actions taken to win the game. The agent’s action space includes only attacking specific grid cells on the board. A reward function will assign positive rewards for successfully hitting and sinking enemy ships, and negative rewards for misses. The probability of hit can be estimated using various machine learning methods such as Monte Carlo and reinforcement learning. The performance can be measured by win and lose rates, as well as the total number of moves the model made. We will repeat simulating the game multiple times to create dataset, making this problem replicable.\n",
    "\n",
    "### State Space (S):\n",
    "At any timestep \\( t \\), the **state** $s_t$ represents the agent’s current knowledge of the board:\n",
    "\n",
    "$$ s_t = \\{ B_t^{seen} \\} $$\n",
    "\n",
    "where:\n",
    "- $B_t^{seen}$ is a **flattened 100-dimensional vector** representing the board, with:\n",
    "  - **1** for a successful hit,\n",
    "  - **-1** for a missed shot,\n",
    "  - **0** for unexplored cells.\n",
    "\n",
    "### Action Space (A):\n",
    "The agent selects an action \\( a_t \\) from the set of available grid cells:\n",
    "\n",
    "$$ A = \\{ (x, y) \\mid x, y \\in \\mathbb{Z}, 1 \\leq x, y \\leq 10, (x, y) \\notin H_t \\cup M_t \\} $$\n",
    "\n",
    "where:\n",
    "- $H_t$ is the set of **hit positions**.\n",
    "- $M_t$ is the set of **missed positions**.\n",
    "Each action $a_t$ corresponds to selecting a **specific cell** \\( (x, y) \\) on the board to attack.\n",
    "\n",
    "### Reward Function (R):\n",
    "The reward function assigns feedback based on the outcome of an action:\n",
    "\n",
    "$$ R(s_t, a_t) = \\begin{cases} \n",
    "+3,  & \\text{if } a_t \\text{ hits an enemy ship} \\\\\n",
    "+10, & \\text{if } a_t \\text{ sinks an enemy ship} \\\\\n",
    "-2,  & \\text{if } a_t \\text{ misses} \\\\\n",
    "-1,  & \\text{for every action taken (cost)}\n",
    "\\end{cases} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We will create our own dataset by simulating Battleship game with variables such as 'models used', 'episodes', 'cost of action', 'cost of misses', and 'reward for hitting'. The critical variables include 'reward received' and 'move selection'. 'Reward received' is going to be represented with the numerical values calculated with reward equation. 'Move selections' are going to be represented with x and y coordinates. The total number of observations is going to be around 3000. The hits for each episode and graphs will be used to visualize the improvement of the model.\n",
    "\n",
    "We are unsure if we will need special handling, transformations, and handling, as we have not produced our data yet, but it will become noticeable whether they are necessary or not as we are planning to create tables and graphs to record and visualize our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "Our proposed solution is to create an algorithm that can perform best when playing the board game, Battleship. We aim to test different reward, action, and state parameters to explore the best or optimal path to win a game of Battleship. This exploration dives into Monte Carlo and policy evaluation approach to improve and calcuate the best moves given the current state in Battleship. Each action take results in a change in the reward function which in turn will determine the next action taken by the model. Over multilate iterations of the model playing Battleships, we aim to view its performance--whether is has won or has found the optimal actions to win a game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "### Cumulative Reward\n",
    "- For each episode, calculate the total reward accumulated by the agent from start to finish.\n",
    "- Plot the total rewards over episodes to evaluate whether the agent is improving its performance over time.\n",
    "\n",
    "### Average Run Reward\n",
    "- Track the total reward for each episode.\n",
    "- Compute the average of rewards over a fixed number of episodes.\n",
    "\n",
    "### Maximum Reward\n",
    "- The highest possible reward achievable in an episode."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "### DQN Shallow Model (1 to 3 layers)\n",
    "\n",
    "To optimize the performance of our DQN model with shallow layers, we conducted hyperparameter tuning by testing various combinations of epsilon (ε), alpha (α), and gamma (γ).\n",
    "\n",
    "\n",
    "1. **Best Performing Combination**:  \n",
    "   - The combination of **ε=0.1, α=0.1, γ=0.8** yielded the best results, with steady improvement in cumulative rewards over episodes. This indicates that a balance of moderate exploration (low ε), stable learning (low α), and a focus on immediate rewards (low γ) is effective for this environment.\n",
    "\n",
    "2. **Effect of Epsilon (ε)**:  \n",
    "   - **Low ε (0.1)**: Encouraged exploitation of known strategies, leading to faster convergence and higher rewards.  \n",
    "   - **High ε (0.3)**: Excessive exploration hindered learning, resulting in poor performance and lower rewards.\n",
    "\n",
    "3. **Effect of Alpha (α)**:  \n",
    "   - **Low α (0.1)**: Provided stable learning with gradual reward improvement.  \n",
    "   - **High α (0.3)**: Caused instability and fluctuating rewards due to aggressive updates.\n",
    "\n",
    "4. **Effect of Gamma (γ)**:  \n",
    "   - **Low γ (0.8)**: Focused on immediate rewards, leading to faster and more effective learning.  \n",
    "   - **High γ (0.9)**: Improved long-term planning but slowed convergence.\n",
    "\n",
    "5. **Worst Combinations**:  \n",
    "   - High ε (0.3) and high α (0.3) resulted in poor performance, with rewards remaining consistently low due to excessive exploration and unstable updates.\n",
    "\n",
    "#### Visualization of Results\n",
    "Below are some reward plots for different hyperparameter combinations:\n",
    "\n",
    "- **Best Combination (ε=0.1, α=0.1, γ=0.8)**:  \n",
    "  ![Best Combination](Results/DQN_Shallow/DQN_epsilon=0.1_alpha=0.1_gamma=0.8.png)\n",
    "\n",
    "- **High Epsilon (ε=0.3, α=0.1, γ=0.9)**:  \n",
    "  ![High Epsilon](Results/DQN_Shallow/DQN_epsilon=0.3_alpha=0.1_gamma=0.9.png)\n",
    "\n",
    "- **High Alpha (ε=0.1, α=0.3, γ=0.9)**:  \n",
    "  ![High Alpha](Results/DQN_Shallow/DQN_epsilon=0.1_alpha=0.3_gamma=0.9.png)\n",
    "\n",
    "- **Worst Combination (ε=0.3, α=0.3, γ=0.8)**:  \n",
    "  ![Worst Combination](Results/DQN_Shallow/DQN_epsilon=0.3_alpha=0.3_gamma=0.8.png)\n",
    "\n",
    "### DQN Deep Model (4 layers)\n",
    "\n",
    "We aim to explore the performance of the model through three different activation functions (Softmax, Leaky ReLU, and Sigmoid) with more hidden (deep) layers of DQN. We will use 4 hidden layers and graph their rewards over 1000 episodes. Epsilon (ε), Alpha (α), and Gamma (γ) will be the best combination and will not change to maintain consistency and highlight the effects of more hidden layers. \n",
    "\n",
    "1. Softmax\n",
    "   - Softmax is used to normalize the output of the network over a probability distribution. Therefore, this allows for easy interpretation and decision-making. Typically, this activation function is used for multi-class classification problems--refering that softmax is not considered ideal in Battleships.\n",
    "\n",
    "2. Leaky ReLU\n",
    "   - Leaky ReLu is similar to ReLU, however allows for small negative gradients. This avoids the common issue with ReLU where some neurons can become permanetly inactive during training.   \n",
    "\n",
    "3. Sigmoid\n",
    "   - Similar to softmax, Sigmoid normalizes the output over a 0-1 probability distribution. However, Sigmoid handles binary classification rather than multi-class.\n",
    "\n",
    "#### Visualization of Results (4 Layers)\n",
    "- **Softmax with best combination (ε=0.1, α=0.1, γ=0.8)**:\n",
    "![Softmax with best combination, ε=0.1, α=0.1, γ=0.8](Results/DQN_Deep/DQNModel_4Layer_softmax_best.png)\n",
    "\n",
    "- **LeakyReLU with best combination (ε=0.1, α=0.1, γ=0.8)**:\n",
    "![Leaky ReLU with best combination, ε=0.1, α=0.1, γ=0.8](Results/DQN_Deep/DQNModel_4Layer_LeReLU_best.png)\n",
    "\n",
    "- **Sigmoid with best combination (ε=0.1, α=0.1, γ=0.8)**:\n",
    "![Sigmoid with best combination, ε=0.1, α=0.1, γ=0.8](Results/DQN_Deep/DQNModel_4Layer_sigmoid_best.png)\n",
    "\n",
    "### DQN with 5 Layers\n",
    "Similarily, this section aims to explore more about the number of layers and see if \"more is better\". Intuitively, more layers would allow the model to discover more features, provide better approximization, and allow for hierarchical learning at the cost of increased computation time. Additionally, more layers can tackle complex feature extraction but can also cause overfitting.\n",
    "\n",
    "- **LeakyReLu with 5 layers (ε=0.1, α=0.1, γ=0.8) and 3000 episodes**:\n",
    "![LeakyReLu with 5 layers](Results/DQN_Deep/DQNModel_5Layer_leakyrelu.png)\n",
    "\n",
    "- **LeakyReLu with 10 layers (ε=0.1, α=0.1, γ=0.8) and 3000 episodes**:\n",
    "![LeakyReLu with 5 layers](Results/DQN_Deep/DQNModel_10Layer_leakyrelu.png)\n",
    "\n",
    "- **Sigmoid with 5 layers (ε=0.1, α=0.1, γ=0.8) with 3000 episodes**:\n",
    "![Sigmoid with 5 layers](Results/DQN_Deep/DQNModel_5Layer_sigmoid.png)\n",
    "\n",
    "### Monte Carlo\n",
    "\n",
    "1. The initial values of the hyperparameters were: ε = 0.1, γ = 0.9, and num_episodes = 20000. The reward distribution remains relatively wide, suggesting inconsistent policy learning.\n",
    "\n",
    "2. By decreasing epsilion over time from ε = 0.1, increasing gamma (γ = 0.99), and increasing number of episodes (30000), a slight improvement of rewards has been shown. To improve on the exploration, we modified epsilon to decay over time, so that the model explores early and exploits later. We made it start at 0.1 (10% exploration rate), then it slowly decrease it with each episode, favoring learned actions over time. A minimum value was set so that it never goes below 0.01 (1% exploration), ensuring some randomness remains. The result showed a wider spread of higher rewards compared to the first graph. However, many episodes resulted in very low rewards (around -150 to -175), meaning the agent is still making many poor moves. This suggests that exploration may be too high for too long, or that the agent is not optimizing actions effectively\n",
    "\n",
    "3. Every reward update was averaged over all past episodes (self.q_table[state][location] = total_return / visit_count). This can cause slow convergence, and old experiences might have too much influence. Therefore, we decided to use alpha (α) to gradually update Q-values instead of full averaging. For example, alpha = 0.1 means 10% of the new information is incorporated each time. Morever, we adjusted the reward values. We set self.hit_ship = 10, so that there is more reward for a hit, self.destroy_ship = 30 to make sinking ships very valuable, and self.hit_empty = -10 to give a stronger penalty for missing. The result still had high variance and the average reward seemed to be negative.\n",
    "\n",
    "#### Visualization of Results\n",
    "Below are some reward plots for different hyperparameter combinations:\n",
    "\n",
    "- **Initial Attempt (ε=0.1, γ=0.9, number of episodes = 20000)**:  \n",
    "  ![Initial Attempt](Results/MC/1.png)\n",
    "\n",
    "- **Second Attempt (ε=0.1, γ=0.99, number of episodes = 30000)**:  \n",
    "  ![Initial Attempt](Results/MC/2.png)\n",
    "\n",
    "- **Third Attempt (ε=0.1, γ=0.99, number of episodes = 30000)**:  \n",
    "  ![Initial Attempt](Results/MC/3.png)\n",
    "\n",
    "### Temporal-Difference \n",
    "The purpose of temporal learning is to allow the agent to maximize total reward by incrementally updating its values of states after actions.\n",
    "\n",
    "1. First Combination: \n",
    "   -(epsilon=0.25, alpha=0.4, gamma=0.98) At a 25% exploration rate, this produced a good balance between exploring new actions and exploting established strategies. There is a high discount factor allowing focus on long term rewards which allows the agent to consider the future outcomes. This has a moderate learning rate allowing steady learning.\n",
    "\n",
    "2. Second Combination(best Combination):\n",
    "   -(epsilon=0.2, alpha=0.5, gamma=0.95) At a 20% exploration rate, this indicates a balanced exploration allowing the agent to explore new stratagies while considering established ones. There is a moderate discount factor that values future rewards but also values immediate rewards which provides a good balance between future and present rewards. There is a moderate learning rate allowing the agent to adapt reasinably. \n",
    "\n",
    "3. Third Combination:\n",
    "   -(epsilon=0.1, alpha=0.7, gamma=0.99) At a 10% exploration rate, the agent exploits on established strategies and takes less new actions which can be useful if the agent is expected to have some knowledge of the environment. There is a higher learning rate which speeds up the agent's adaptation which can help the agent adjust its policy based upon new experiences. This combination has a high discount factor which places value on future rewards suitable if the agent is planning on long term outcomes. \n",
    "\n",
    "4. Fourth Combination:\n",
    "   -(epsilon=0,4, alpha=0.3, gamma=0.9) At a 40% exploration rate, this shows the agent makes random actions quite often encouraging exploration of environment and learning new strategies. There is a moderate discount factor indicating a balance between new and current rewards suitable for short term and long term rewards. This combination provides a slow learning rate which slows down the agent's learning process that can prevent overfitting but leads to slower convergence. \n",
    "\n",
    "5. Fifth Combination:\n",
    "   -(epsilon=0.05, alpha=0.8, gamma=0.85) This is a very low exploration rate showing the agent is most likely exploting learned actions and less likely to take random actions. This is also a low discount factor which prioritizes immediate rewards over long term outcomes. This combination has a higher learning rate which allows the agent to learn quickly which may lead to rapid environment adaptations but may be risky. \n",
    "\n",
    "6. Sixth Combination:\n",
    "   -(epsilon=0.3, alpha=0.6, gamma=0.85) Set at a 30% exploration rate, this exhibits a good amount of exploration and exploitation which allows the agent to explore new actions while accounting for established strategies. This has a low discount factor which values immediate rewards as opposed to future outcomes which can be important if quick rewards are important. This has a moderate learning rate encouraging quick updates to agent's policy while maintaining stability. \n",
    "\n",
    "\n",
    "#### Visualization of Results|\n",
    "\n",
    "- **First Combination**:  \n",
    "  ![First Combination](Results/TD/1.png)\n",
    "\n",
    "- **Second Combination**:  \n",
    "  ![Second Combination](Results/TD/2.png)\n",
    "\n",
    "- **Third Combination**:  \n",
    "  ![Third Combination](Results/TD/3.png)\n",
    "\n",
    "- **Fourth Combination**:  \n",
    "  ![Fourth Combination](Results/TD/4.png)\n",
    "\n",
    "- **Fifth Combination**:  \n",
    "  ![Fifth Combination](Results/TD/5.png)\n",
    "\n",
    "- **Sixth Combination**:  \n",
    "  ![Sixth Combination](Results/TD/6.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "The results suggest that balancing exploration, learning rate, and reward prioritization is key to effective reinforcement learning. Too much exploration can prevent the model from stabilizing on good strategies, while a high learning rate may cause erratic updates, leading to instability.\n",
    "\n",
    "- **DQN Shallow**: The results show that ε=0.1, α=0.1, γ=0.8 provided relatively stable learning, while increasing γ to 0.9 led to slightly slower convergence. A higher α (0.3) introduced instability, causing more fluctuations in rewards. Increasing ε to 0.3 led to excessive exploration, preventing effective policy learning. The worst performance occurred with high ε and high α, where learning was erratic with low rewards.\n",
    "\n",
    "- **DQN Deep**:  While exploring DQN with deep hidden layers, we found it to be true that more is not always better. However, having deep hidden layers allows for more exploration and feature extraction that can be helpful if the model learns general strategies rather than fixed patterns. The complexity of battleship and all possible states in the game would justify that DQN is preferred for decision-making. Additionally, activation functions also play a role such that if we want a distribution of probabilities or probability estimate then we can use either softmax or sigmoid. LeakyReLU is simple and avoids the negative side effects of ReLU where we can have \"dead\" neurons. Each of the activation functions explored resulted in negative rewards, suggesting multiple possible reasons for that cause.\n",
    "\n",
    "- **TD Learning**: The model improves when there is a moderate balance of exploration and exploitation while maintaining a moderate discount factor to value established strategies while making future actions.\n",
    "\n",
    "- **Monte Carlo**: Increasing the number of episodes, epsilon decay strategy, and increasing gamma(γ) value resulted in the most significant improvement. However, the presence of many extremely low-reward episodes (-150 to -175) indicated that the agent was still making frequent poor decisions, possibly due to prolonged exploration or inefficient action optimization.\n",
    "\n",
    "\n",
    "### Limitations\n",
    "\n",
    "The state space is extremely large (3¹⁰⁰), making it computationally expensive to analyze using NumPy arrays for state representation. To improve efficiency, we used dictionaries where keys represent the current board state and values indicate the next hit location, thereby optimizing the representation of the state and action space. However, this approach introduces significant fluctuations in rewards. Additionally, because the ship's placement is randomized in each episode, a substantial number of episodes may be required to observe meaningful improvements in performance. The learning rate is another limitation that can lead to slow learning. As a result, it can be instable and possibly overshoot the optimal value. Another limitation is the computation time and the cost to run more episodes. Likewise, the game environment meant that the reward is sparce or noisy which could explain why each model did not reach a high reward. In Monte Carlo, the result of the third attempt shows that the epsilon decay helped balancing the exploration and exploitation, the agent still struggled with optimizing actions. The decay rate may have been too slow or fast for the training. Another limitation found in the monte carlo model is that it had a high variance in rewards, suggesting that it may not be ideal for long-term dependencies in state-action values.\n",
    "\n",
    "\n",
    "### Future work\n",
    "For future work, training the model through the game environment can be simplified, such as starting with a simpler board configuration and gradually increase complexity to improve learning efficiency. Evaluation methods and performance metrics can be defined clearly to evaluate the performance of Battleship models. Other works can focus on updating the reward and loss function or include epsilon decay when running DQN. We also suggest to explore mixing different activation layers. As an alternative to epsilon decay, we may want to try using adaptive exploration methods such as Upper Confidence Bound (UCB) or Boltzmann exploration, which are likely to help the agent to decide whether it needs to explore or exploit. Another possible change we could make is implementing importance sampling or weighted Q-value updates to stabilize learning by prioritizing recent experiences.\n",
    "\n",
    "### Ethics & Privacy\n",
    "Our potential concerns root from data security, honest representation, miss application, cheating, and metric selection. We will keep our data secure by encrypting stored game data and AI models to prevent unauthorized access. Furthermore, we will honesty represent our data by checking for unrealistic ship placement. To avoid miss application, where the model trained on Battleship has a possibility of adapting for other purposes, we will clearly document that the model is intended only for Battleship strategy optimization. Another ethical concern regarding this research is that our model could be used to cheat on Battleship games, which makes the game unfair. Lastly, to select the most relevant metrics for our problem, we will carefully design our reward function, making sure the model is prioritizing actions that lead to a win as well as making efficient moves.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The results suggest that balancing exploration, learning rate, and reward prioritization is key to effective reinforcement learning. Too much exploration can prevent the model from stabilizing on good strategies, while a high learning rate may cause erratic updates, leading to instability. All of the models were tested with several combinations ranging from low to high levels of exploration and learning. Each model(DQN, MC, and TD) performed the most optimal results when testing for moderate exploration rates and moderate learning rates. Each model was demonstarting minor instability when produced with a high learning rate and was erratic with a high exploration rate. Establishing a balance between confirmed actions and future actions yielded the most ideal results for battleship performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<p id=\"footnote1\">[1] <a href=\"https://www.ga-ccri.com/deep-reinforcement-learning-win-battleship\" target=\"_blank\">He, S. (2017). *Deep Reinforcement Learning–of how to win at Battleship*. GA-CCRi</a>.</p>\n",
    "\n",
    "<p id=\"footnote1\">[2] <a href=\"https://is.muni.cz/th/oupp1/Reinforcement_Learning_for_the_Game_of_Battleship.pdf\" target=\"_blank\">Šebek, P. (2020). *Reinforcement Learning for the Game of Battleship*. Masaryk University</a>.</p>\n",
    "\n",
    "<p id=\"footnote1\">[3] <a href=\"https://medium.com/towards-data-science/an-artificial-intelligence-learns-to-play-battleship-ebd2cf9adb01\" target=\"_blank\">An Artificial Intelligence Learns to Play Battleship</a>.</p>\n",
    "\n",
    "<p id=\"footnote1\">[4] <a href=https://www.geeksforgeeks.org/play-battleships-game-with-ai/#1-probability-density-function target=\"_blank\">Play Battleships Game with AI: Insights into Algorithmic Strategies and Game Mastery</a>.</p>\n",
    "\n",
    "<p id=\"footnote1\">[5] <a href=https://towardsdatascience.com/coding-an-intelligent-battleship-agent-bf0064a4b319/ target=\"_blank\">Coding an Intelligent Battleship Agent</a>.</p> \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
