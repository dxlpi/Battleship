{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Battleship\n",
    "[Group \"Battleship\" Repo](https://github.com/dxlpi/Battleship)\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Hyeonseo (Heather) Jang\n",
    "- Bonan (Jack) Jia\n",
    "- Esha Kakarla\n",
    "- Kevin Tran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "This section should be short and clearly stated. It should be a single paragraph <200 words.  It should summarize: \n",
    "- what your goal/problem is\n",
    "- what the data used represents \n",
    "- the solution/what you did\n",
    "- major results you came up with (mention how results are measured) \n",
    "\n",
    "__NB:__ this final project form is much more report-like than the proposal and the checkpoint. Think in terms of writing a paper with bits of code in the middle to make the plots/tables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Battleship is a popular naval strategy game that is somewhat simple but has served as a testing ground for algorithms involving AI development and Reinforcement Learning. Battleship is a great well established environment for reinforcement learning because it encompasses a mix of strategic decisions, sequential actions, and hidden information between players <sup><a href=\"#footnote1\">[1]</a></sup>.\n",
    "\n",
    "The paper Reinforcement Learning for the Game of Battleship explores reinforcement learning algorithms in battleship. This paper analyzes certain challenges with the game's partially observable environment where the model has to to deduct the locations of the \n",
    "opponent's ships through several test-runs. This paper also explores heuristic search algorithms and analyzes the performances between the decision making processes <sup><a href=\"#footnote1\">[2]</a></sup>.\n",
    "\n",
    "Another article titled, An Artificial Intelligence Learns to Play Battleship on Medium, discusses the method of reinforcement learning for a game of battleship. This project has two main approaches, code from scratch using a linear model and using openAi gym library with neural networks. This author implemented q-learning to improve the agent's decision making over 100,000 training episodes<sup><a href=\"#footnote1\">[3]</a></sup>.\n",
    "\n",
    "Developing an AI approach to play battleship can involve several different types of algorithms to locate ships and stragetize. Common approaches are Monte Carlo Tree Search, Reinforcement Learning, and probability density function. By leveraging these algorithms, the search method can be refined over time and improve decision making <sup><a href=\"#footnote1\">[4]</a></sup>.\n",
    "\n",
    "The article, Coding an Intelligent Battleship Agent, discusses creating an agent to solve the Battleship game, from simple random guessing to advanced approaches such as a Hunt/Target strategy and probabilistic strategies. The agent has learned to compile shots based on probabilities improving overall efficiency <sup><a href=\"#footnote1\">[5]</a></sup>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The problem we are trying to solve is: “How can we develop an optimal strategy for winning the Battleship game using machine learning?” The winner of Battleship is the first person who sinks all of the enemy ships with the least number of actions taken. As such, we are exploring how to optimize the number of actions taken to win the game. The agent’s action space includes only attacking specific grid cells on the board. A reward function will assign positive rewards for successfully hitting and sinking enemy ships, and negative rewards for misses. The probability of hit can be estimated using various machine learning methods such as Monte Carlo and reinforcement learning. The performance can be measured by win and lose rates, as well as the total number of moves the model made. We will repeat simulating the game multiple times to create dataset, making this problem replicable.\n",
    "\n",
    "**comments**: Make it more mathematical and precise (ie. describe the reward function mathematically using an equation, describe the agent's stae and action spce completely in mathematical terms.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We will create our own dataset by simulating Battleship game with variables such as 'models used', 'episodes', 'cost of action', 'cost of misses', and 'reward for hitting'. The critical variables include 'reward received' and 'move selection'. 'Reward received' is going to be represented with the numerical values calculated with reward equation. 'Move selections' are going to be represented with x and y coordinates. The total number of observations is going to be around 3000. The hits for each episode and graphs will be used to visualize the improvement of the model.\n",
    "\n",
    "We are unsure if we will need special handling, transformations, and handling, as we have not produced our data yet, but it will become noticeable whether they are necessary or not as we are planning to create tables and graphs to record and visualize our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "Our proposed solution is to create an algorithm that can perform best when playing the board game, Battleship. We aim to test different reward, action, and state parameters to explore the best or optimal path to win a game of Battleship. This exploration dives into Monte Carlo and policy evaluation approach to improve and calcuate the best moves given the current state in Battleship. Each action take results in a change in the reward function which in turn will determine the next action taken by the model. Over multilate iterations of the model playing Battleships, we aim to view its performance--whether is has won or has found the optimal actions to win a game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "### Cumulative Reward\n",
    "- For each episode, calculate the total reward accumulated by the agent from start to finish.\n",
    "- Plot the total rewards over episodes to evaluate whether the agent is improving its performance over time.\n",
    "\n",
    "### Average Run Reward\n",
    "- Track the total reward for each episode.\n",
    "- Compute the average of rewards over a fixed number of episodes.\n",
    "\n",
    "### Maximum Reward\n",
    "- The highest possible reward achievable in an episode."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "### DQN Shallow Model (1 to 3 layers)\n",
    "\n",
    "To optimize the performance of our DQN model with shallow layers, we conducted hyperparameter tuning by testing various combinations of epsilon (ε), alpha (α), and gamma (γ).\n",
    "\n",
    "\n",
    "1. **Best Performing Combination**:  \n",
    "   - The combination of **ε=0.1, α=0.1, γ=0.8** yielded the best results, with steady improvement in cumulative rewards over episodes. This indicates that a balance of moderate exploration (low ε), stable learning (low α), and a focus on immediate rewards (low γ) is effective for this environment.\n",
    "\n",
    "2. **Effect of Epsilon (ε)**:  \n",
    "   - **Low ε (0.1)**: Encouraged exploitation of known strategies, leading to faster convergence and higher rewards.  \n",
    "   - **High ε (0.3)**: Excessive exploration hindered learning, resulting in poor performance and lower rewards.\n",
    "\n",
    "3. **Effect of Alpha (α)**:  \n",
    "   - **Low α (0.1)**: Provided stable learning with gradual reward improvement.  \n",
    "   - **High α (0.3)**: Caused instability and fluctuating rewards due to aggressive updates.\n",
    "\n",
    "4. **Effect of Gamma (γ)**:  \n",
    "   - **Low γ (0.8)**: Focused on immediate rewards, leading to faster and more effective learning.  \n",
    "   - **High γ (0.9)**: Improved long-term planning but slowed convergence.\n",
    "\n",
    "5. **Worst Combinations**:  \n",
    "   - High ε (0.3) and high α (0.3) resulted in poor performance, with rewards remaining consistently low due to excessive exploration and unstable updates.\n",
    "\n",
    "#### Visualization of Results\n",
    "Below are some reward plots for different hyperparameter combinations:\n",
    "\n",
    "- **Best Combination (ε=0.1, α=0.1, γ=0.8)**:  \n",
    "  ![Best Combination](Results/DQN_Shallow/DQN_epsilon=0.1_alpha=0.1_gamma=0.8.png)\n",
    "\n",
    "- **High Epsilon (ε=0.3, α=0.1, γ=0.9)**:  \n",
    "  ![High Epsilon](Results/DQN_Shallow/DQN_epsilon=0.3_alpha=0.1_gamma=0.9.png)\n",
    "\n",
    "- **High Alpha (ε=0.1, α=0.3, γ=0.9)**:  \n",
    "  ![High Alpha](Results/DQN_Shallow/DQN_epsilon=0.1_alpha=0.3_gamma=0.9.png)\n",
    "\n",
    "- **Worst Combination (ε=0.3, α=0.3, γ=0.8)**:  \n",
    "  ![Worst Combination](Results/DQN_Shallow/DQN_epsilon=0.3_alpha=0.3_gamma=0.8.png)\n",
    "\n",
    "### DQN Deep Model (4 layers)\n",
    "\n",
    "We aim to explore the performance of the model through three different activation functions (Softmax, Leaky ReLU, and Sigmoid) with more hidden (deep) layers of DQN. We will use 4 hidden layers and graph their rewards over 1000 episodes. Epsilon (ε), Alpha (α), and Gamma (γ) will be the best combination and will not change to maintain consistency and highlight the effects of more hidden layers. \n",
    "\n",
    "1. Softmax\n",
    "   - Softmax is used to normalize the output of the network over a probability distribution. Therefore, this allows for easy interpretation and decision-making. Typically, this activation function is used for multi-class classification problems--refering that softmax is not considered ideal in Battleships.\n",
    "\n",
    "2. Leaky ReLU\n",
    "   - Leaky ReLu is similar to ReLU, however allows for small negative gradients. This avoids the common issue with ReLU where some neurons can become permanetly inactive during training.   \n",
    "\n",
    "3. Sigmoid\n",
    "   - Similar to softmax, Sigmoid normalizes the output over a 0-1 probability distribution. However, Sigmoid handles binary classification rather than multi-class.\n",
    "\n",
    "#### Visualization of Results (4 Layers)\n",
    "- **Softmax with best combination (ε=0.1, α=0.1, γ=0.8)**:\n",
    "![Softmax with best combination, ε=0.1, α=0.1, γ=0.8](Results/DQN_Deep/DQNModel_4Layer_softmax_best.png)\n",
    "\n",
    "- **LeakyReLU with best combination (ε=0.1, α=0.1, γ=0.8)**:\n",
    "![Leaky ReLU with best combination, ε=0.1, α=0.1, γ=0.8](Results/DQN_Deep/DQNModel_4Layer_LeReLU_best.png)\n",
    "\n",
    "- **Sigmoid with best combination (ε=0.1, α=0.1, γ=0.8)**:\n",
    "![Sigmoid with best combination, ε=0.1, α=0.1, γ=0.8](Results/DQN_Deep/DQNModel_4Layer_sigmoid_best.png)\n",
    "\n",
    "### DQN with 5 Layers\n",
    "Similarily, this section aims to explore more about the number of layers and see if \"more is better\". Intuitively, more layers would allow the model to discover more features, provide better approximization, and allow for hierarchical learning at the cost of increased computation time. Additionally, more layers can tackle complex feature extraction but can also cause overfitting.\n",
    "\n",
    "- **LeakyReLu with 5 layers (ε=0.1, α=0.1, γ=0.8) and 3000 episodes**:\n",
    "![LeakyReLu with 5 layers](Results/DQN_Deep/DQNModel_5Layer_leakyrelu.png)\n",
    "\n",
    "- **LeakyReLu with 10 layers (ε=0.1, α=0.1, γ=0.8) and 3000 episodes**:\n",
    "![LeakyReLu with 5 layers](Results/DQN_Deep/DQNModel_10Layer_leakyrelu.png)\n",
    "\n",
    "- **Sigmoid with 5 layers (ε=0.1, α=0.1, γ=0.8) with 3000 episodes**:\n",
    "![Sigmoid with 5 layers](Results/DQN_Deep/DQNModel_5Layer_sigmoid.png)\n",
    "\n",
    "\n",
    "While exploring DQN with deep hidden layers, we found it to be true that more is not always better. However, having deep hidden layers allows for more exploration and feature extraction that can be helpful if the model learns general strategies rather than fixed patterns. The complexity of battleship and all possible states in the game would justify that DQN is preferred for decision-making. Additionally, activation functions also play a role such that if we want a distribution of probabilities or probability estimate then we can use either softmax or sigmoid. LeakyReLU is simple and avoids the negative side effects of ReLU where we can have \"dead\" neurons. Each of the activation functions explored resulted in negative rewards, suggesting multiple possible reasons for that cause.\n",
    "\n",
    "\n",
    "### Monte Carlo\n",
    "\n",
    "1. The initial values of the hyperparameters were: ε = 0.1, γ = 0.9, and num_episodes = 20000. The reward distribution remains relatively wide, suggesting inconsistent policy learning.\n",
    "\n",
    "2. By decreasing epsilion over time from ε = 0.1, increasing gamma (γ = 0.99), and increasing number of episodes (30000), a slight improvement of rewards has been shown. To improve on the exploration, we modified epsilon to decay over time, so that the model explores early and exploits later. We made it start at 0.1 (10% exploration rate), then it slowly decrease it with each episode, favoring learned actions over time. A minimum value was set so that it never goes below 0.01 (1% exploration), ensuring some randomness remains. The result showed a wider spread of higher rewards compared to the first graph. However, many episodes resulted in very low rewards (around -150 to -175), meaning the agent is still making many poor moves. This suggests that exploration may be too high for too long, or that the agent is not optimizing actions effectively\n",
    "\n",
    "3. Every reward update was averaged over all past episodes (self.q_table[state][location] = total_return / visit_count). This can cause slow convergence, and old experiences might have too much influence. Therefore, we decided to use alpha (α) to gradually update Q-values instead of full averaging. For example, alpha = 0.1 means 10% of the new information is incorporated each time. Morever, we adjusted the reward values. We set self.hit_ship = 10, so that there is more reward for a hit, self.destroy_ship = 30 to make sinking ships very valuable, and self.hit_empty = -10 to give a stronger penalty for missing. The result still had high variance and the average reward seemed to be negative.\n",
    "\n",
    "#### Visualization of Results\n",
    "Below are some reward plots for different hyperparameter combinations:\n",
    "\n",
    "- **Initial Attempt (ε=0.1, γ=0.9, number of episodes = 20000)**:  \n",
    "  ![Initial Attempt](Results/MC/1.png)\n",
    "\n",
    "- **Second Attempt (ε=0.1, γ=0.99, number of episodes = 30000)**:  \n",
    "  ![Initial Attempt](Results/MC/2.png)\n",
    "\n",
    "- **Third Attempt (ε=0.1, γ=0.99, number of episodes = 30000)**:  \n",
    "  ![Initial Attempt](Results/MC/3.png)\n",
    "\n",
    "#### Conclusion for Monte Carlo\n",
    "Throughout adjusting the hyperparameters of Monte Carlo model, we found out that increasing the number of episodes, epsilon decay strategy, and increasing gamma(γ) value resulted in the most significant improvement. However, the presence of many extremely low-reward episodes (-150 to -175) indicated that the agent was still making frequent poor decisions, possibly due to prolonged exploration or inefficient action optimization.\n",
    "\n",
    "### Temporal-Difference \n",
    "The purpose of temporal learning is to allow the agent to maximize total reward by incrementally updating its values of states after actions.\n",
    "\n",
    "1. First Combination: \n",
    "   -(epsilon=0.25, alpha=0.4, gamma=0.98) At a 25% exploration rate, this produced a good balance between exploring new actions and exploting established strategies. There is a high discount factor allowing focus on long term rewards which allows the agent to consider the future outcomes. This has a moderate learning rate allowing steady learning.\n",
    "\n",
    "2. Second Combination(best Combination):\n",
    "   -(epsilon=0.2, alpha=0.5, gamma=0.95) At a 20% exploration rate, this indicates a balanced exploration allowing the agent to explore new stratagies while considering established ones. There is a moderate discount factor that values future rewards but also values immediate rewards which provides a good balance between future and present rewards. There is a moderate learning rate allowing the agent to adapt reasinably. \n",
    "\n",
    "3. Third Combination:\n",
    "   -(epsilon=0.1, alpha=0.7, gamma=0.99) At a 10% exploration rate, the agent exploits on established strategies and takes less new actions which can be useful if the agent is expected to have some knowledge of the environment. There is a higher learning rate which speeds up the agent's adaptation which can help the agent adjust its policy based upon new experiences. This combination has a high discount factor which places value on future rewards suitable if the agent is planning on long term outcomes. \n",
    "\n",
    "4. Fourth Combination:\n",
    "   -(epsilon=0,4, alpha=0.3, gamma=0.9) At a 40% exploration rate, this shows the agent makes random actions quite often encouraging exploration of environment and learning new strategies. There is a moderate discount factor indicating a balance between new and current rewards suitable for short term and long term rewards. This combination provides a slow learning rate which slows down the agent's learning process that can prevent overfitting but leads to slower convergence. \n",
    "\n",
    "5. Fifth Combination:\n",
    "   -(epsilon=0.05, alpha=0.8, gamma=0.85) This is a very low exploration rate showing the agent is most likely exploting learned actions and less likely to take random actions. This is also a low discount factor which prioritizes immediate rewards over long term outcomes. This combination has a higher learning rate which allows the agent to learn quickly which may lead to rapid environment adaptations but may be risky. \n",
    "\n",
    "6. Sixth Combination:\n",
    "   -(epsilon=0.3, alpha=0.6, gamma=0.85) Set at a 30% exploration rate, this exhibits a good amount of exploration and exploitation which allows the agent to explore new actions while accounting for established strategies. This has a low discount factor which values immediate rewards as opposed to future outcomes which can be important if quick rewards are important. This has a moderate learning rate encouraging quick updates to agent's policy while maintaining stability. \n",
    "\n",
    "\n",
    "#### Visualization of Results|\n",
    "\n",
    "- **First Combination**:  \n",
    "  ![First Combination](Results/TD/1.png)\n",
    "\n",
    "- **Second Combination**:  \n",
    "  ![Second Combination](Results/TD/2.png)\n",
    "\n",
    "- **Third Combination**:  \n",
    "  ![Third Combination](Results/TD/3.png)\n",
    "\n",
    "- **Fourth Combination**:  \n",
    "  ![Fourth Combination](Results/TD/4.png)\n",
    "\n",
    "- **Fifth Combination**:  \n",
    "  ![Fifth Combination](Results/TD/5.png)\n",
    "\n",
    "- **Sixth Combination**:  \n",
    "  ![Sixth Combination](Results/TD/6.png)\n",
    "\n",
    "#### TD Learning Conclusion\n",
    "We found that having a moderate exploration rate lead to a balanced decision making by the agent for taking new actions and taking established strategies into account. Having a higher discount rate (gamma) allows the agent to place more value into future rewards which leads to improvement. Having a moderate learning rate (alpha) leads to a balance of adaptation without risk of overshooting or instability. \n",
    "### Subsection 5 \n",
    "\n",
    "Maybe you do model selection again, but using a different kind of metric than before?  Or you compare a completely different approach/alogirhtm to the problem? Whatever, this stuff is just serving suggestions.\n",
    "\n",
    "**Note**: For all images related to the results, please refer to the `Results` folder. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Are there any problems with the work?  For instance would more data change the nature of the problem? Would it be good to explore more hyperparams than you had time for?   \n",
    "\n",
    "\n",
    "### Future work\n",
    "Looking at the limitations and/or the toughest parts of the problem and/or the situations where the algorithm(s) did the worst... is there something you'd like to try to make these better.\n",
    "\n",
    "### Ethics & Privacy\n",
    "Our potential concerns root from data security, honest representation, miss application, and metric selection. We will keep our data secure by encrypting stored game data and AI models to prevent unauthorized access. Furthermore, we will honesty represent our data by checking for unrealistic ship placement. To avoid miss application, where the model trained on Battleship has a possibility of adapting for other purposes, we will clearly document that the model is intended only for Battleship strategy optimization. Lastly, to select the most relevant metrics for our problem, we will carefully design our reward function, making sure the model is prioritizing actions that lead to a win as well as making efficient moves. \n",
    "\n",
    "**comments**: If extend to more complex strategic games, could similar AI be used for automated cheating in multiplayer games? While Battleship is relatively simple, AI-assisted cheating is an ethical concern in competitive gaming. There should be clear guidelines for the use of unauthorized AI in competitive gaming. \n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<p id=\"footnote1\">[1] <a href=\"https://www.ga-ccri.com/deep-reinforcement-learning-win-battleship\" target=\"_blank\">He, S. (2017). *Deep Reinforcement Learning–of how to win at Battleship*. GA-CCRi</a>.</p>\n",
    "\n",
    "<p id=\"footnote1\">[2] <a href=\"https://is.muni.cz/th/oupp1/Reinforcement_Learning_for_the_Game_of_Battleship.pdf\" target=\"_blank\">Šebek, P. (2020). *Reinforcement Learning for the Game of Battleship*. Masaryk University</a>.</p>\n",
    "\n",
    "<p id=\"footnote1\">[3] <a href=\"https://medium.com/towards-data-science/an-artificial-intelligence-learns-to-play-battleship-ebd2cf9adb01\" target=\"_blank\">An Artificial Intelligence Learns to Play Battleship</a>.</p>\n",
    "\n",
    "<p id=\"footnote1\">[4] <a href=https://www.geeksforgeeks.org/play-battleships-game-with-ai/#1-probability-density-function target=\"_blank\">Play Battleships Game with AI: Insights into Algorithmic Strategies and Game Mastery</a>.</p>\n",
    "\n",
    "<p id=\"footnote1\">[5] <a href=https://towardsdatascience.com/coding-an-intelligent-battleship-agent-bf0064a4b319/ target=\"_blank\">Coding an Intelligent Battleship Agent</a>.</p> \n",
    "\n",
    "<a name=\"admonishnote\"></a>2.[^](#admonish): Also refs should be important to the background, not some randomly chosen vaguely related stuff. Include a web link if possible in refs as above.<br>\n",
    "<a name=\"sotanote\"></a>3.[^](#sota): Perhaps the current state of the art solution such as you see on [Papers with code](https://paperswithcode.com/sota). Or maybe not SOTA, but rather a standard textbook/Kaggle solution to this kind of problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
